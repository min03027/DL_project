{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTBk7r4d+xBvCMdDzEPbDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/min03027/start/blob/main/DL_Perceptron_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제1. 단일층 퍼셉트론 구현\n",
        "요구사항\n",
        "- 입력데이터는 3차원 벡터\n",
        "- 가중치와 바이어스는 임의로 초기화\n",
        "- 활성화 함수는 시그모이드 사용\n",
        "- 입력 데이터에 대한 출력을 계산"
      ],
      "metadata": {
        "id": "OgL7QV9ChSsd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oVWlj6mbEMP",
        "outputId": "4917d23c-83e5-4e27-8717-46a02f864338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perceptron output :  [0.99376058]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 함수선언\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# 데이터 입력\n",
        "inputs = np.array([1,2,3])\n",
        "\n",
        "#가중치와 편향을 임의로 초기화\n",
        "np.random.seed(42)\n",
        "weights = np.random.rand(3)\n",
        "bias = np.random.rand(1)\n",
        "\n",
        "# 입력데이터에 대한 출력을 계산\n",
        "output = sigmoid(np.dot(inputs,weights)+bias)\n",
        "\n",
        "#결과 출력\n",
        "print(\"perceptron output : \",output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제2. 다중 퍼셉트론과 순전파 구현\n",
        "\n",
        "요구사항\n",
        "- 입력데이터는 4차원 벡터\n",
        "- 첫번째 은닉층은 5개의 노드를, 두번째 은닉층은 3개의 노드를 가진다.\n",
        "- 가중치와 바이어스는 임의로 초기화\n",
        "- 활성화 함수는 시그모이드 사용\n",
        "- 입력 데이터에 대한 출력을 계산"
      ],
      "metadata": {
        "id": "dAVgDl60k6fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 활성화함수 정의\n",
        "def sigmoid (x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# 입력데이터\n",
        "inputs = ([1,2,3,4])\n",
        "\n",
        "# 가중치와 편향 정의\n",
        "np.random.seed(42)\n",
        "weights_input_hidden1 = np.random.rand(4,5)\n",
        "bias_hidden1 = np.random.rand(1,5)\n",
        "weights_hidden1_hidden2 = np.random.rand(5,3)\n",
        "bias_hidden2 = np.random.rand(1,3)\n",
        "weights_hidden2_output= np.random.rand(3,1)\n",
        "bias_output = np.random.rand(1,1)\n",
        "\n",
        "#순전파\n",
        "hidden_layer_activation = np.dot(inputs,weights_input_hidden1)+bias_hidden1\n",
        "hidden_layer1_output = sigmoid(hidden_layer_activation)\n",
        "hidden_layer2_activation = np.dot(hidden_layer1_output,weights_hidden1_hidden2)+bias_hidden2\n",
        "hidden_layer2_output = sigmoid(hidden_layer2_activation)\n",
        "final_input= np.dot(hidden_layer2_output,weights_hidden2_output)+bias_output\n",
        "final_output = sigmoid(final_input)\n",
        "\n",
        "#프린트\n",
        "print(\"final predicted\", final_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxOFuKk6ieeF",
        "outputId": "6f56d451-9829-463c-c6c7-5984518ab35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final predicted [[0.88265813]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제3. 역전파를 이용한 신경망 학습\n",
        "1. 다층 퍼셉트론에서의 오차계산\n",
        "2. 역전파 알고리즘을 통해 가중치와 편향 계산\n",
        "\n",
        "\n",
        "요구사항\n",
        "- 입력데이터는 2차원 벡터\n",
        "- 첫번째 은닉층은 4개의 노드를, 출력층은 1개의 노드를 가진다.\n",
        "- 가중치와 바이어스는 임의로 초기화\n",
        "- 활성화 함수는 시그모이드 사용\n",
        "- 주어진 출력 데이터의 오차 계산, 역전파 알고리즘을 통한 가중치와 편향 계산\n",
        "- 학습률을 0.1로 설정"
      ],
      "metadata": {
        "id": "qnr7_OmbtNIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "inputs = np.array([[1,2]])\n",
        "actual_output = np.array([[1]])\n",
        "\n",
        "np.random.seed(42)\n",
        "weights_input = np.random.rand(2,4)\n",
        "bias_input = np.random.rand(1,4)\n",
        "weights_hidden1 = np.random.rand(4,1)\n",
        "bias_hidden1 = np.random.rand(1,1)\n",
        "\n",
        "# 순전파\n",
        "input_activation = np.dot(inputs,weights_input)+bias_input\n",
        "input_result = sigmoid(input_activation)\n",
        "hidden1_activation = np.dot(input_result,weights_hidden1)+bias_hidden1\n",
        "hidden1_result = sigmoid(hidden1_activation)\n",
        "\n",
        "#오차비교\n",
        "error = actual_output- hidden1_result\n",
        "d_predicted_output = error*sigmoid_derivative(hidden1_activation)\n",
        "error_hidden_layer = d_predicted_output.dot(hidden1_result.T)\n",
        "d_hidden_layer = error_hidden_layer*sigmoid_derivative(hidden1_activation)\n",
        "\n",
        "# 가중치와 편향 업데이트\n",
        "learning_rate = 0.1\n",
        "weights_hidden1 += input_result.T.dot(d_predicted_output)*learning_rate\n",
        "bias_hidden1 += np.sum(d_predicted_output,axis=0,keepdims =True)*learning_rate\n",
        "weights_input +=inputs.T.dot(d_hidden_layer)*learning_rate\n",
        "bias_input +=np.sum(d_hidden_layer,axis=0,keepdims=True)*learning_rate\n",
        "\n",
        "print(\"updated weights from input to hidden layer : \",weights_input)\n",
        "print(\"Updated bias of hidden layer:\\n\", bias_input)\n",
        "print(\"Updated weights from hidden to output layer:\\n\", weights_hidden1)\n",
        "print(\"Updated bias of output layer:\\n\", bias_hidden1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxqpO-w9ntTy",
        "outputId": "755cf98d-e519-445a-f667-848c8b308af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated weights from input to hidden layer :  [[0.37490598 0.95108017 0.73235981 0.59902435]\n",
            " [0.15675037 0.15672625 0.05881534 0.86690788]]\n",
            "Updated bias of hidden layer:\n",
            " [[0.60148088 0.70843844 0.02095036 0.97027572]]\n",
            "Updated weights from hidden to output layer:\n",
            " [[0.83474094]\n",
            " [0.21491288]\n",
            " [0.18389081]\n",
            " [0.18623271]]\n",
            "Updated bias of output layer:\n",
            " [[0.30717466]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제4. 간단한 2층 신경망의 오차역전파법 구현\n",
        "1. 입력데이터 [1,0][1,0][1,0]\n",
        "2. 실제 출력값 [1][1][1]\n",
        "\n",
        "\n",
        "요구사항\n",
        "- 입력데이터는 2개의 노드\n",
        "- 첫번째 은닉층은 2개의 노드\n",
        "- 출력층은 1개의 노드\n",
        "- 가중치와 바이어스는 임의로 초기화\n",
        "- 활성화 함수는 시그모이드 사용\n",
        "- 주어진 출력 데이터의 오차 계산, 역전파 알고리즘을 통한 가중치와 편향 계산\n",
        "- 학습률을 0.1로 설정\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qHLSdUbZ4E1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 작성코드\n",
        "import numpy as np\n",
        "\n",
        "#함수정의\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "#입력\n",
        "input= np.array([[1,0],[1,0],[1,0]])\n",
        "actual_output = np.array([[1],[1],[1]])\n",
        "\n",
        "# 가중치와 편향\n",
        "np.random.seed(42)\n",
        "weights_input = np.random.rand(2,2)\n",
        "bias_input = np.random.rand(1,2)\n",
        "weights_hidden1 = np.random.rand(2,1)\n",
        "bias_hidden1 = np.random.rand(1,1)\n",
        "weights_output = np.random.rand(1,1)\n",
        "bias_output = np.random.rand(1,1)\n",
        "\n",
        "#순전파\n",
        "input_activation = np.dot(input,weights_input)+bias_input\n",
        "input_result = sigmoid(input_activation)\n",
        "hidden_activation = np.dot(input_result,weights_hidden1)+bias_hidden1\n",
        "hidden_result = sigmoid(hidden_activation)\n",
        "output_activtion = np.dot(hidden_result,weights_output)+bias_output\n",
        "output_result = sigmoid(output_activtion)\n",
        "\n",
        "#오차계산\n",
        "error = actual_output - output_result\n",
        "d_predicted_output = error * sigmoid_derivative(output_result)\n",
        "error_hidden = d_predicted_output.dot(weights_output.T)\n",
        "d_hidden = error_hidden * sigmoid_derivative(hidden_result)\n",
        "\n",
        "#가중치와 편향 업데이트\n",
        "learning_rate = 0.1\n",
        "weights_output += hidden_result.T.dot(d_predicted_output) * learning_rate\n",
        "bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "weights_hidden1 += input_result.T.dot(d_hidden) * learning_rate\n",
        "bias_hidden1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "weights_input += input.T.dot(d_hidden.dot(weights_hidden1.T)) * learning_rate\n",
        "bias_input += np.sum(d_hidden.dot(weights_hidden1.T) * sigmoid_derivative(input_result), axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "\n",
        "\n",
        "print(\"Updated weights from input to first hidden layer:\", weights_input)\n",
        "print(\"Updated bias of first hidden layer:\", bias_input)\n",
        "print(\"Updated weights from first hidden layer to second hidden layer:\", weights_hidden1)\n",
        "print(\"Updated bias of second hidden layer:\", bias_hidden1)\n",
        "print(\"Updated weights from second hidden layer to output layer:\", weights_output)\n",
        "print(\"Updated bias of output layer:\", bias_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwzELtFd4Gr5",
        "outputId": "fca7568c-567d-45f8-9cd3-c3af99e1ff20"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights from input to first hidden layer: [[0.37476487 0.95394641]\n",
            " [0.73199394 0.59865848]]\n",
            "Updated bias of first hidden layer: [[0.15606961 0.1566984 ]]\n",
            "Updated weights from first hidden layer to second hidden layer: [[0.06042543]\n",
            " [0.86897138]]\n",
            "Updated bias of second hidden layer: [[0.60483447]]\n",
            "Updated weights from second hidden layer to output layer: [[0.72720679]]\n",
            "Updated bias of output layer: [[0.04499302]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수정코드\n",
        "import numpy as np\n",
        "\n",
        "# 함수 정의\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# 입력\n",
        "input_data = np.array([[1, 0], [1, 0], [1, 0]])\n",
        "actual_output = np.array([[1], [1], [1]])\n",
        "\n",
        "# 가중치와 편향 초기화\n",
        "np.random.seed(42)\n",
        "weights_input = np.random.rand(2, 2)\n",
        "bias_input = np.random.rand(1, 2)\n",
        "weights_hidden1 = np.random.rand(2, 1)\n",
        "bias_hidden1 = np.random.rand(1, 1)\n",
        "weights_output = np.random.rand(1, 1)\n",
        "bias_output = np.random.rand(1, 1)\n",
        "\n",
        "# 순전파\n",
        "input_activation = np.dot(input_data, weights_input) + bias_input\n",
        "input_result = sigmoid(input_activation)\n",
        "hidden_activation = np.dot(input_result, weights_hidden1) + bias_hidden1\n",
        "hidden_result = sigmoid(hidden_activation)\n",
        "output_activation = np.dot(hidden_result, weights_output) + bias_output\n",
        "output_result = sigmoid(output_activation)\n",
        "\n",
        "# 오차 계산\n",
        "error = actual_output - output_result\n",
        "d_predicted_output = error * sigmoid_derivative(output_result)\n",
        "error_hidden = d_predicted_output.dot(weights_output.T)\n",
        "d_hidden = error_hidden * sigmoid_derivative(hidden_result)\n",
        "\n",
        "# 가중치와 편향 업데이트\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Output layer\n",
        "weights_output += hidden_result.T.dot(d_predicted_output) * learning_rate\n",
        "bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# Hidden layer\n",
        "weights_hidden1 += input_result.T.dot(d_hidden) * learning_rate\n",
        "bias_hidden1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# Input layer\n",
        "weights_input += input_data.T.dot(d_hidden.dot(weights_hidden1.T) * sigmoid_derivative(input_result)) * learning_rate\n",
        "bias_input += np.sum(d_hidden.dot(weights_hidden1.T) * sigmoid_derivative(input_result), axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "print(\"Updated weights from input to first hidden layer:\", weights_input)\n",
        "print(\"Updated bias of first hidden layer:\", bias_input)\n",
        "print(\"Updated weights from first hidden layer to output layer:\", weights_hidden1)\n",
        "print(\"Updated bias of hidden layer:\", bias_hidden1)\n",
        "print(\"Updated weights from hidden layer to output layer:\", weights_output)\n",
        "print(\"Updated bias of output layer:\", bias_output)\n"
      ],
      "metadata": {
        "id": "d0YLvPn29d6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7310dac1-b40f-48e7-c15e-f27505a2af2a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights from input to first hidden layer: [[0.37459109 0.95141818]\n",
            " [0.73199394 0.59865848]]\n",
            "Updated bias of first hidden layer: [[0.15606961 0.1566984 ]]\n",
            "Updated weights from first hidden layer to output layer: [[0.06042543]\n",
            " [0.86897138]]\n",
            "Updated bias of hidden layer: [[0.60483447]]\n",
            "Updated weights from hidden layer to output layer: [[0.72720679]]\n",
            "Updated bias of output layer: [[0.04499302]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다른방식\n",
        "import numpy as np\n",
        "\n",
        "# 함수 정의\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "x = np.array([[1,0],[1,0],[1,0]])\n",
        "y = np.array([[1],[1],[1]])\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "w1 = np.random.rand(2,2)\n",
        "b1 = np.random.rand(1,2)\n",
        "w2 = np.random.rand(2,1)\n",
        "b2 = np.random.rand(1,1)\n",
        "\n",
        "\n",
        "hidden_acti= np.dot(x,w1) + b1 # 활성화\n",
        "hidden_output = sigmoid(hidden_acti)\n",
        "output_acti = np.dot(hidden_output,w2) + b2\n",
        "output_output = sigmoid(output_acti)\n",
        "\n",
        "error = y - output_output\n",
        "d_predicted_output = error * sigmoid_derivative(output_acti)\n",
        "\n",
        "error_hidden_layer = d_predicted_output.dot(w2.T)\n",
        "d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_acti)\n",
        "\n",
        "learning_rate = 0.1\n",
        "w2 += hidden_output.T.dot(d_predicted_output) * learning_rate\n",
        "b2 += np.sum(d_predicted_output, axis = 0, keepdims = True) * learning_rate\n",
        "w1 += x.T.dot(d_hidden_layer) * learning_rate\n",
        "b1 += np.sum(d_hidden_layer, axis = 0, keepdims = True) * learning_rate\n",
        "\n",
        "print(\"Updated weights from input to hidden layer :\\n\", w1)\n",
        "print(\"Updated bias of hidden layer 1:\\n\", b1)\n",
        "print(\"Updated weights from hidden layer to output layer:\\n\", w2)\n",
        "print(\"Updated bias of output layer :\\n\", b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCOIzET9Giy7",
        "outputId": "c0a1c9d9-618b-4d4c-db05-79f29df95193"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights from input to hidden layer :\n",
            " [[0.37468886 0.95249047]\n",
            " [0.73199394 0.59865848]]\n",
            "Updated bias of hidden layer 1:\n",
            " [[0.15616738 0.15777068]]\n",
            "Updated weights from hidden layer to output layer:\n",
            " [[0.06499735]\n",
            " [0.87442848]]\n",
            "Updated bias of output layer :\n",
            " [[0.61209594]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제5. 3층 신경망의 오차역전파법 구현\n",
        "1. 입력데이터 [0.5,0.1,0.2][0.5,0.1,0.2][0.5,0.1,0.2]\n",
        "2. 실제 출력값 [0.7,0.3][0.7,0.3][0.7,0.3]\n",
        "\n",
        "\n",
        "요구사항\n",
        "- 입력데이터는 3개의 노드\n",
        "- 첫번째 은닉층은 4개의 노드\n",
        "- 두번째 은닉층은 3개의 노드\n",
        "- 출력층은 2개의 노드\n",
        "- 가중치와 바이어스는 임의로 초기화\n",
        "- 활성화 함수는 시그모이드 사용\n",
        "- 주어진 출력 데이터의 오차 계산, 역전파 알고리즘을 통한 가중치와 편향 계산\n",
        "- 학습률을 0.1로 설정\n"
      ],
      "metadata": {
        "id": "zjRXv4NVE0hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 함수 정의\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "x = np.array([[0.5,0.1,0.2],[0.5,0.1,0.2],[0.5,0.1,0.2]])\n",
        "y = np.array([[0.7,0.3],[0.7,0.3],[0.7,0.3]])\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "w1 = np.random.rand(3,4)\n",
        "b1 = np.random.rand(1,4)\n",
        "w2 = np.random.rand(4,3)\n",
        "b2 = np.random.rand(1,3)\n",
        "w3 = np.random.rand(3,2)\n",
        "b3 = np.random.rand(1,2)\n",
        "\n",
        "hidden_acti= np.dot(x,w1) + b1 # 활성화\n",
        "hidden_output = sigmoid(hidden_acti)\n",
        "hidden2_acti = np.dot(hidden_output,w2) + b2\n",
        "hidden2_output = sigmoid(hidden2_acti)\n",
        "output_acti = np.dot(hidden2_output,w3) + b3\n",
        "output_output = sigmoid(output_acti)\n",
        "\n",
        "#오차계산\n",
        "error = y - output_output\n",
        "d_predicted_output = error * sigmoid_derivative(output_acti)\n",
        "\n",
        "error_hidden2 = d_predicted_output.dot(w3.T)\n",
        "d_hidden2 = error_hidden2 * sigmoid_derivative(hidden2_acti)\n",
        "\n",
        "error_hidden1 = d_hidden2.dot(w2.T)\n",
        "d_hidden1 = error_hidden1 * sigmoid_derivative(hidden_acti)\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "w3 += hidden2_output.T.dot(d_predicted_output) * learning_rate\n",
        "b3 += np.sum(d_predicted_output, axis = 0, keepdims = True) * learning_rate\n",
        "w2 += hidden_output.T.dot(d_hidden2) * learning_rate\n",
        "b2 += np.sum(d_hidden2, axis = 0, keepdims = True) * learning_rate\n",
        "w1 += x.T.dot(d_hidden1) * learning_rate\n",
        "b1 += np.sum(d_hidden1, axis = 0, keepdims = True) * learning_rate\n",
        "\n",
        "print(\"Updated weights from input to hidden layer 1:\\n\", w1)\n",
        "print(\"Updated bias of hidden layer 1:\\n\", b1)\n",
        "print(\"Updated weights from hidden layer 1 to hidden layer 2:\\n\", w2)\n",
        "print(\"Updated bias of hidden layer 2:\\n\", b2)\n",
        "print(\"Updated weights from hidden layer 2 to output layer:\\n\", w3)\n",
        "print(\"Updated bias of output layer:\\n\", b3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJZoc7A3EpwR",
        "outputId": "efabea7b-8e01-4891-8b1c-01a1754b5650"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights from input to hidden layer 1:\n",
            " [[0.37426242 0.95039822 0.73171876 0.59845621]\n",
            " [0.1559631  0.1559313  0.05802858 0.86613569]\n",
            " [0.60100393 0.70794614 0.02047442 0.96982894]]\n",
            "Updated bias of hidden layer 1:\n",
            " [[0.83188724 0.21170695 0.18127461 0.18299995]]\n",
            "Updated weights from hidden layer 1 to hidden layer 2:\n",
            " [[0.30401908 0.52137663 0.43083896]\n",
            " [0.29102388 0.60874429 0.13847655]\n",
            " [0.29195804 0.36353573 0.45514513]\n",
            " [0.78497582 0.19664259 0.51324247]]\n",
            "Updated bias of hidden layer 2:\n",
            " [[0.59212113 0.04200636 0.60609051]]\n",
            "Updated weights from hidden layer 2 to output layer:\n",
            " [[0.16571334 0.04722376]\n",
            " [0.9445027  0.94939008]\n",
            " [0.80364104 0.28698781]]\n",
            "Updated bias of output layer:\n",
            " [[0.09202438 0.6633036 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제6. 다중클래스 분류를 위한 3층 신경망의 오차역전파법 구현\n",
        "1. 입력데이터 [0.2,0.4,0.6,0.8][0.2,0.4,0.6,0.8][0.2,0.4,0.6,0.8]\n",
        "2. 실제 출력값 [0,1,0][0,1,0][0,1,0]\n",
        "\n",
        "\n",
        "요구사항\n",
        "- 입력데이터는 4개의 노드\n",
        "- 첫번째 은닉층은 5개의 노드\n",
        "- 두번째 은닉층은 4개의 노드\n",
        "- 출력층은 3개의 노드 / 소프트맥스 활성화 함수\n",
        "- 가중치와 바이어스는 임의로 초기화\n",
        "- 활성화 함수는 시그모이드 사용(출력층 제외)\n",
        "- 주어진 출력 데이터을 이용한 (원-핫인코딩)을 토한 오차계산\n",
        "- 오차 역전파법을 통한 가중치와 편향 계산\n",
        "- 학습률을 0.1로 설정\n"
      ],
      "metadata": {
        "id": "ax4ALy7cJ6D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 함수 정의\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    # 크로스 엔트로피 손실 함수\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10), axis=-1)\n",
        "\n",
        "def softmax_derivative(y_true, y_pred):\n",
        "    # 소프트맥스 함수와 크로스 엔트로피 손실 함수의 결합된 미분\n",
        "    return y_pred - y_true\n",
        "\n",
        "\n",
        "x = np.array([[0.2,0.4,0.6,0.8],[0.2,0.4,0.6,0.8],[0.2,0.4,0.6,0.8]])\n",
        "y = np.array([[0,1,0],[0,1,0],[0,1,0]])\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "w1 = np.random.rand(4,5)\n",
        "b1 = np.random.rand(1,5)\n",
        "w2 = np.random.rand(5,4)\n",
        "b2 = np.random.rand(1,4)\n",
        "w3 = np.random.rand(4,3)\n",
        "b3 = np.random.rand(1,3)\n",
        "\n",
        "hidden_acti= np.dot(x,w1) + b1 # 활성화\n",
        "hidden_output = sigmoid(hidden_acti)\n",
        "hidden2_acti = np.dot(hidden_output,w2) + b2\n",
        "hidden2_output = sigmoid(hidden2_acti)\n",
        "output_acti = np.dot(hidden2_output,w3) + b3\n",
        "output_output = softmax(output_acti)\n",
        "\n",
        "#오차계산\n",
        "error = softmax_derivative(y, output_output)  # 수정된 부분\n",
        "d_predicted_output = error\n",
        "\n",
        "error_hidden2 = d_predicted_output.dot(w3.T)\n",
        "d_hidden2 = error_hidden2 * sigmoid_derivative(hidden2_acti)\n",
        "\n",
        "error_hidden1 = d_hidden2.dot(w2.T)\n",
        "d_hidden1 = error_hidden1 * sigmoid_derivative(hidden_acti)\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "w3 += hidden2_output.T.dot(d_predicted_output) * learning_rate\n",
        "b3 += np.sum(d_predicted_output, axis = 0, keepdims = True) * learning_rate\n",
        "w2 += hidden_output.T.dot(d_hidden2) * learning_rate\n",
        "b2 += np.sum(d_hidden2, axis = 0, keepdims = True) * learning_rate\n",
        "w1 += x.T.dot(d_hidden1) * learning_rate\n",
        "b1 += np.sum(d_hidden1, axis = 0, keepdims = True) * learning_rate\n",
        "\n",
        "print(\"Updated weights from input to hidden layer 1:\\n\", w1)\n",
        "print(\"Updated bias of hidden layer 1:\\n\", b1)\n",
        "print(\"Updated weights from hidden layer 1 to hidden layer 2:\\n\", w2)\n",
        "print(\"Updated bias of hidden layer 2:\\n\", b2)\n",
        "print(\"Updated weights from hidden layer 2 to output layer:\\n\", w3)\n",
        "print(\"Updated bias of output layer:\\n\", b3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAqFZ1vkIdd-",
        "outputId": "cba03356-13d1-4e35-f97d-f1724c0da529"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights from input to hidden layer 1:\n",
            " [[0.37439102 0.95069311 0.73189755 0.59865879 0.15603834]\n",
            " [0.15569632 0.05804122 0.86598336 0.60111562 0.70811197]\n",
            " [0.0201372  0.96984626 0.83215346 0.21234003 0.18188405]\n",
            " [0.18280811 0.30415745 0.52437085 0.43194624 0.29130792]]\n",
            "Updated bias of hidden layer 1:\n",
            " [[0.6111074  0.13938787 0.29166268 0.36636337 0.45616846]]\n",
            "Updated weights from hidden layer 1 to hidden layer 2:\n",
            " [[0.78151649 0.19862287 0.51717238 0.59068959]\n",
            " [0.04252398 0.60641728 0.17367639 0.06320078]\n",
            " [0.94454076 0.96438432 0.81188547 0.30256576]\n",
            " [0.09372612 0.68309984 0.44332046 0.1201782 ]\n",
            " [0.49131218 0.03327867 0.91242313 0.25695825]]\n",
            "Updated bias of hidden layer 2:\n",
            " [[0.65738718 0.3102364  0.52419064 0.54428973]]\n",
            "Updated weights from hidden layer 2 to output layer:\n",
            " [[ 0.25553788  0.83339001  0.84064402]\n",
            " [ 1.00889074  0.76112147  0.66221406]\n",
            " [ 0.99360384 -0.04971792  0.26246368]\n",
            " [ 0.10844598  0.20351894  0.44726998]]\n",
            "Updated bias of output layer:\n",
            " [[0.34778734 0.68145423 0.42759829]]\n"
          ]
        }
      ]
    }
  ]
}